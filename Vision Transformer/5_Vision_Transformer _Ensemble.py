# -*- coding: utf-8 -*-
"""State Of the Art.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n814rpwoDjNNqt4xcOQivaH9IamvC43g
"""

!pip install timm

import timm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from torchvision import transforms
import numpy as np
import os

dataset_path = "/kaggle/input/binary-ulcerative-colitis/Binary Ulceraive Colitis"

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

full_dataset = ImageFolder(root=dataset_path, transform=transform)
train_size = int(0.7 * len(full_dataset))
val_size = int(0.2 * len(full_dataset))
test_size = len(full_dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
models = {
    "vit_base_16": timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2),
    "vit_large_32": timm.create_model('vit_large_patch32_224', pretrained=True, num_classes=2),
    "deit_small_16": timm.create_model('deit_small_patch16_224', pretrained=True, num_classes=2),
    "swin_tiny": timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=2),
    "deit_base": timm.create_model('deit_base_patch16_224', pretrained=True, num_classes=2)
}

for model_name in models:
    models[model_name].to(device)

criterion = nn.CrossEntropyLoss()
optimizers = {
    name: torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)
    for name, model in models.items()
}

def fine_tune_model(model, optimizer, train_loader, val_loader, model_name, epochs=200, patience=70):
    best_val_loss = float('inf')
    early_stop_counter = 0
    checkpoint_path = f"{model_name}_best_model.pth"

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        print(f"[{model_name}] Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            early_stop_counter = 0
            torch.save(model.state_dict(), checkpoint_path)
            print(f"[{model_name}] Best model saved at epoch {epoch+1}")
        else:
            early_stop_counter += 1

        if early_stop_counter >= patience:
            print(f"[{model_name}] Early stopping triggered!")
            break

    model.load_state_dict(torch.load(checkpoint_path))
    return model

for name, model in models.items():
    print(f"\nFine-tuning {name}...")
    models[name] = fine_tune_model(model, optimizers[name], train_loader, val_loader, model_name=name)

def soft_voting(models, test_loader):
    all_preds = []

    for name, model in models.items():
        model.eval()
        preds = []

        with torch.no_grad():
            for images, _ in test_loader:
                images = images.to(device)
                outputs = model(images)
                probs = F.softmax(outputs, dim=1)
                preds.append(probs.cpu().numpy())

        all_preds.append(np.concatenate(preds, axis=0))

    avg_probs = np.mean(all_preds, axis=0)
    final_preds = np.argmax(avg_probs, axis=1)

    return final_preds

true_labels = []
for _, labels in test_loader:
    true_labels.extend(labels.numpy())

final_predictions = soft_voting(models, test_loader)

accuracy = np.mean(np.array(final_predictions) == np.array(true_labels))
print(f"Test Accuracy (Soft Voting): {accuracy * 100:.2f}%")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

true_labels = []
for _, labels in test_loader:
    true_labels.extend(labels.numpy())

final_predictions = soft_voting(models, test_loader)

report = classification_report(true_labels, final_predictions, target_names=['Mild', 'Severe'])
print("Classification Report:\n", report)

conf_matrix = confusion_matrix(true_labels, final_predictions)
print("Confusion Matrix:\n", conf_matrix)

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef
import numpy as np
import matplotlib.pyplot as plt

true_labels = []
for _, labels in test_loader:
    true_labels.extend(labels.numpy())
true_labels = np.array(true_labels)

final_predictions = soft_voting(models, test_loader)

all_probs = []
for images, _ in test_loader:
    images = images.to(device)
    model_probs = [F.softmax(model(images), dim=1).detach().cpu().numpy() for model in models.values()]
    avg_probs = np.mean(model_probs, axis=0)
    all_probs.append(avg_probs)

all_probs = np.concatenate(all_probs, axis=0)

positive_probs = all_probs[:, 1]

report = classification_report(true_labels, final_predictions, target_names=['Mild', 'Severe'])
print("Classification Report:\n", report)

conf_matrix = confusion_matrix(true_labels, final_predictions)
print("Confusion Matrix:\n", conf_matrix)

mcc = matthews_corrcoef(true_labels, final_predictions)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

tn, fp, fn, tp = conf_matrix.ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
print(f"Sensitivity: {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

auc_score = roc_auc_score(true_labels, positive_probs)
print(f"AUC Score: {auc_score:.4f}")

fpr, tpr, thresholds = roc_curve(true_labels, positive_probs)
plt.figure()
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auc_score:.4f})")
plt.plot([0, 1], [0, 1], 'k--', label="Random guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef
import numpy as np

conf_matrix = confusion_matrix(true_labels, final_predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Mild', 'Severe'], yticklabels=['Mild', 'Severe'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

