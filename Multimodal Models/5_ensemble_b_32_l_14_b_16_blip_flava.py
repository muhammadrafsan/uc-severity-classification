# -*- coding: utf-8 -*-
"""all-ensemble-b-32-l-14-b-16-blip-flava.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4oJNjVfq0q7-f0OZC9lzlTU3lFKcfad
"""

import os
import random
from sklearn.model_selection import train_test_split

mild_path = '/kaggle/input/ulcerative-colitis-binary-dataset/Binary Ulceraive Colitis/mild'
severe_path = '/kaggle/input/ulcerative-colitis-binary-dataset/Binary Ulceraive Colitis/severe'

# Get the list of images for both classes
mild_images = [os.path.join(mild_path, img) for img in os.listdir(mild_path)]
severe_images = [os.path.join(severe_path, img) for img in os.listdir(severe_path)]

# Combine the datasets and create labels (0 for mild, 1 for severe)
all_images = mild_images + severe_images
all_labels = [0] * len(mild_images) + [1] * len(severe_images)

# Split the dataset into 80% train and 20% test
train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.1, stratify=all_labels, random_state=42)

# Print the number of images in train and test sets
print(f"Total training images: {len(train_images)}")
print(f"Total test images: {len(test_images)}")

# Install CLIP library
!pip install git+https://github.com/openai/CLIP.git

import torch
import clip
from PIL import Image
from tqdm import tqdm

# Load CLIP ViT-B/32 model and preprocess
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_b32_model, clip_b32_preprocess = clip.load("ViT-B/32", device=device)

# Load CLIP ViT-L/14 model and preprocess
clip_l14_model, clip_l14_preprocess = clip.load("ViT-L/14", device=device)

# Load CLIP ViT-B/16 model and preprocess
clip_b16_model, clip_b16_preprocess = clip.load("ViT-B/16", device=device)

# Function to encode images using a specified CLIP model and preprocess
def encode_images_with_clip(images_list, model, preprocess):
    features = []
    with torch.no_grad():
        for image_path in tqdm(images_list):
            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
            image_features = model.encode_image(image)
            features.append(image_features.cpu())
    return torch.cat(features)

# Encode training and test images using CLIP ViT-B/32
clip_b32_train_features = encode_images_with_clip(train_images, clip_b32_model, clip_b32_preprocess)
clip_b32_test_features = encode_images_with_clip(test_images, clip_b32_model, clip_b32_preprocess)

# Encode training and test images using CLIP ViT-L/14
clip_l14_train_features = encode_images_with_clip(train_images, clip_l14_model, clip_l14_preprocess)
clip_l14_test_features = encode_images_with_clip(test_images, clip_l14_model, clip_l14_preprocess)

# Encode training and test images using CLIP ViT-B/16
clip_b16_train_features = encode_images_with_clip(train_images, clip_b16_model, clip_b16_preprocess)
clip_b16_test_features = encode_images_with_clip(test_images, clip_b16_model, clip_b16_preprocess)

# Save the features for later use
torch.save(clip_b32_train_features, "clip_b32_train_features.pt")
torch.save(clip_b32_test_features, "clip_b32_test_features.pt")
torch.save(clip_l14_train_features, "clip_l14_train_features.pt")
torch.save(clip_l14_test_features, "clip_l14_test_features.pt")
torch.save(clip_b16_train_features, "clip_b16_train_features.pt")
torch.save(clip_b16_test_features, "clip_b16_test_features.pt")
print("Feature extraction completed for all CLIP models.")

!pip install transformers

from transformers import BlipProcessor, BlipModel

# Load BLIP model and processor
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def encode_images_blip(images_list):
    features = []
    with torch.no_grad():
        for image_path in tqdm(images_list):
            image = Image.open(image_path)
            inputs = processor(images=image, return_tensors="pt").to(device)
            image_features = model.get_image_features(**inputs)
            features.append(image_features.cpu())
    return torch.cat(features)

blip_train_features = encode_images_blip(train_images)
blip_test_features = encode_images_blip(test_images)

torch.cuda.empty_cache()

torch.save(blip_train_features, "blip_train_features.pt")

torch.save(blip_test_features, "blip_test_features.pt")

print("BLIP feature extraction completed for training images.")
print("BLIP test feature extraction completed.")

from transformers import FlavaProcessor, FlavaModel
device = "cuda" if torch.cuda.is_available() else "cpu"

processor = FlavaProcessor.from_pretrained("facebook/flava-full")
model = FlavaModel.from_pretrained("facebook/flava-full").to(device)

def encode_images_flava(images_list, batch_size=8):
    features = []
    with torch.no_grad():
        for i in tqdm(range(0, len(images_list), batch_size)):
            batch_images = images_list[i:i + batch_size]
            batch_features = []
            for image_path in batch_images:
                image = Image.open(image_path)
                inputs = processor(images=image, return_tensors="pt").to(device)
                image_features = model.get_image_features(**inputs)
                batch_features.append(image_features.cpu())
            features.append(torch.cat(batch_features))
    return torch.cat(features)

flava_train_features = encode_images_flava(train_images, batch_size=8)
flava_test_features = encode_images_flava(test_images, batch_size = 8)

torch.save(flava_test_features, "flava_test_features.pt")

print("FLAVA test feature extraction completed.")

torch.save(flava_train_features, "flava_train_features.pt")



print("FLAVA feature extraction completed for training images.")
print("FLAVA test feature extraction completed.")

torch.cuda.empty_cache()

clip_b32_train_features = torch.load("clip_b32_train_features.pt")
clip_l14_train_features = torch.load("clip_l14_train_features.pt")
clip_b16_train_features = torch.load("clip_b16_train_features.pt")
blip_train_features = torch.load("blip_train_features.pt")
flava_train_features = torch.load("flava_train_features.pt")

def compute_class_prototypes(features, labels):
    mild_features = []
    severe_features = []

    for feature, label in zip(features, labels):
        if label == 0:
            mild_features.append(feature)
        else:
            severe_features.append(feature)

    mild_prototype = torch.mean(torch.stack(mild_features), dim=0)
    severe_prototype = torch.mean(torch.stack(severe_features), dim=0)

    return mild_prototype, severe_prototype

clip_b32_mild_prototype, clip_b32_severe_prototype = compute_class_prototypes(clip_b32_train_features, train_labels)
clip_l14_mild_prototype, clip_l14_severe_prototype = compute_class_prototypes(clip_l14_train_features, train_labels)
clip_b16_mild_prototype, clip_b16_severe_prototype = compute_class_prototypes(clip_b16_train_features, train_labels)
blip_mild_prototype, blip_severe_prototype = compute_class_prototypes(blip_train_features, train_labels)
flava_mild_prototype, flava_severe_prototype = compute_class_prototypes(flava_train_features, train_labels)

blip_test_features = torch.load("blip_test_features.pt")
flava_test_features = torch.load("flava_test_features.pt")

import torch.nn.functional as F
import numpy as np

def classify_with_threshold(test_features, mild_prototype, severe_prototype, threshold=0.5):
    predictions = []
    for feature in test_features:
        mild_similarity = F.cosine_similarity(feature, mild_prototype.unsqueeze(0)).mean()
        severe_similarity = F.cosine_similarity(feature, severe_prototype.unsqueeze(0)).mean()
        normalized_similarity = mild_similarity / (mild_similarity + severe_similarity)
        predictions.append(0 if normalized_similarity > threshold else 1)
    return predictions

clip_b32_predictions = classify_with_threshold(clip_b32_test_features, clip_b32_mild_prototype, clip_b32_severe_prototype)
clip_l14_predictions = classify_with_threshold(clip_l14_test_features, clip_l14_mild_prototype, clip_l14_severe_prototype)
clip_b16_predictions = classify_with_threshold(clip_b16_test_features, clip_b16_mild_prototype, clip_b16_severe_prototype)
blip_predictions = classify_with_threshold(blip_test_features, blip_mild_prototype, blip_severe_prototype, threshold=0.5)
flava_predictions = classify_with_threshold(flava_test_features, flava_mild_prototype, flava_severe_prototype, threshold=0.5)

def soft_voting(clip_b32_preds, clip_l14_preds, clip_b16_preds, blip_preds, flava_preds):
    final_predictions = []
    for i in range(len(clip_b32_preds)):
        votes = np.array([clip_b32_preds[i], clip_l14_preds[i], clip_b16_preds[i], blip_preds[i], flava_preds[i]])
        final_predictions.append(np.bincount(votes).argmax())
    return final_predictions

final_predictions = soft_voting(clip_b32_predictions, clip_l14_predictions, clip_b16_predictions, blip_predictions, flava_predictions)
print("Final predictions:", final_predictions)

from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(test_labels, final_predictions)
print(f"Soft Voting Accuracy: {accuracy * 100:.2f}%")
report = classification_report(test_labels, final_predictions, target_names=["Mild", "Severe"])
print(report)

def weighted_voting(clip_b32_preds, clip_l14_preds, clip_b16_preds, blip_preds, flava_preds, weights):
    final_predictions = []
    for i in range(len(clip_b32_preds)):
        votes = np.concatenate((
            [clip_b32_preds[i]] * weights[0],
            [clip_l14_preds[i]] * weights[1],
            [clip_b16_preds[i]] * weights[2],
            [blip_preds[i]] * weights[3],
            [flava_preds[i]] * weights[4]
        ))
        final_predictions.append(np.bincount(votes).argmax())
    return final_predictions

final_predictions_weighted = weighted_voting(clip_b32_predictions, clip_l14_predictions, clip_b16_predictions,blip_predictions,flava_predictions, weights=[2, 2, 1,2,2])

accuracy_weighted = accuracy_score(test_labels, final_predictions_weighted)
print(f"Weighted Voting Accuracy: {accuracy_weighted * 100:.2f}%")
report_weighted = classification_report(test_labels, final_predictions_weighted, target_names=["Mild", "Severe"])
print(report_weighted)



























