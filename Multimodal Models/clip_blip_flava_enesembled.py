# -*- coding: utf-8 -*-
"""clip-blip-flava-enesembled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_TCZjc1AZWFr5tdpBTVBlVOTsTQZBwx
"""

!pip install git+https://github.com/openai/CLIP.git

!pip install transformers

import os
import random
from sklearn.model_selection import train_test_split
mild_path = '/kaggle/input/ulcerative-colitis-binary-dataset/Binary Ulceraive Colitis/mild'
severe_path = '/kaggle/input/ulcerative-colitis-binary-dataset/Binary Ulceraive Colitis/severe'
mild_images = [os.path.join(mild_path, img) for img in os.listdir(mild_path)]
severe_images = [os.path.join(severe_path, img) for img in os.listdir(severe_path)]
all_images = mild_images + severe_images
all_labels = [0] * len(mild_images) + [1] * len(severe_images)
train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.1, stratify=all_labels, random_state=42)
print(f"Total training images: {len(train_images)}")
print(f"Total test images: {len(test_images)}")

import torch
import clip
from PIL import Image
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
device

def encode_images(images_list):
    features = []
    with torch.no_grad():
        for image_path in tqdm(images_list):
            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
            image_features = model.encode_image(image)
            features.append(image_features.cpu())
    return torch.cat(features)

clip_train_features = encode_images(train_images)
clip_test_features = encode_images(test_images)

torch.cuda.empty_cache()

torch.save(clip_train_features, "clip_train_features.pt")
torch.save(clip_test_features, "clip_test_features.pt")
print("CLIP feature extraction completed for training images.")

from transformers import BlipProcessor, BlipModel
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def encode_images_blip(images_list):
    features = []
    with torch.no_grad():
        for image_path in tqdm(images_list):
            image = Image.open(image_path)
            inputs = processor(images=image, return_tensors="pt").to(device)
            image_features = model.get_image_features(**inputs)
            features.append(image_features.cpu())
    return torch.cat(features)

blip_train_features = encode_images_blip(train_images)
blip_test_features = encode_images_blip(test_images)

torch.cuda.empty_cache()

torch.save(blip_train_features, "blip_train_features.pt")
torch.save(blip_test_features, "blip_test_features.pt")
print("BLIP feature extraction completed for training images.")
print("BLIP test feature extraction completed.")

from transformers import FlavaProcessor, FlavaModel

device = "cuda" if torch.cuda.is_available() else "cpu"
processor = FlavaProcessor.from_pretrained("facebook/flava-full")
model = FlavaModel.from_pretrained("facebook/flava-full").to(device)

def encode_images_flava(images_list, batch_size=8):
    features = []
    with torch.no_grad():
        for i in tqdm(range(0, len(images_list), batch_size)):
            batch_images = images_list[i:i + batch_size]
            batch_features = []
            for image_path in batch_images:
                image = Image.open(image_path)
                inputs = processor(images=image, return_tensors="pt").to(device)
                image_features = model.get_image_features(**inputs)
                batch_features.append(image_features.cpu())
            features.append(torch.cat(batch_features))
    return torch.cat(features)

flava_train_features = encode_images_flava(train_images, batch_size=8)
flava_test_features = encode_images_flava(test_images, batch_size = 8)
torch.save(flava_test_features, "flava_test_features.pt")
print("FLAVA test feature extraction completed.")

torch.cuda.empty_cache()

torch.save(flava_train_features, "flava_train_features.pt")
torch.save(flava_test_features, "flava_test_features.pt")
print("FLAVA feature extraction completed for training images.")
print("FLAVA test feature extraction completed.")

def compute_class_prototypes(features, labels):
    mild_features = []
    severe_features = []

    for feature, label in zip(features, labels):
        if label == 0:
            mild_features.append(feature)
        else:
            severe_features.append(feature)
    mild_prototype = torch.mean(torch.stack(mild_features), dim=0)
    severe_prototype = torch.mean(torch.stack(severe_features), dim=0)

    return mild_prototype, severe_prototype

clip_train_features = torch.load("clip_train_features.pt")
blip_train_features = torch.load("blip_train_features.pt")
flava_train_features = torch.load("flava_train_features.pt")

clip_mild_prototype, clip_severe_prototype = compute_class_prototypes(clip_train_features, train_labels)
blip_mild_prototype, blip_severe_prototype = compute_class_prototypes(blip_train_features, train_labels)
flava_mild_prototype, flava_severe_prototype = compute_class_prototypes(flava_train_features, train_labels)

import torch.nn.functional as F

def classify_with_threshold(test_features, mild_prototype, severe_prototype, threshold=0.5):
    predictions = []
    for feature in test_features:
        mild_similarity = F.cosine_similarity(feature, mild_prototype.unsqueeze(0)).mean()
        severe_similarity = F.cosine_similarity(feature, severe_prototype.unsqueeze(0)).mean()
        normalized_similarity = mild_similarity / (mild_similarity + severe_similarity)
        if normalized_similarity > threshold:
            predictions.append(0)
        else:
            predictions.append(1)
    return predictions

clip_test_features = torch.load("clip_test_features.pt")
blip_test_features = torch.load("blip_test_features.pt")
flava_test_features = torch.load("flava_test_features.pt")

clip_predictions = classify_with_threshold(clip_test_features, clip_mild_prototype, clip_severe_prototype, threshold=0.5)
blip_predictions = classify_with_threshold(blip_test_features, blip_mild_prototype, blip_severe_prototype, threshold=0.5)
flava_predictions = classify_with_threshold(flava_test_features, flava_mild_prototype, flava_severe_prototype, threshold=0.5)

import numpy as np

def soft_voting(clip_preds, blip_preds, flava_preds):
    final_predictions = []
    for i in range(len(clip_preds)):
        votes = np.array([clip_preds[i], blip_preds[i], flava_preds[i]])
        final_predictions.append(np.bincount(votes).argmax())
    return final_predictions

final_predictions = soft_voting(clip_predictions, blip_predictions, flava_predictions)
print("Final predictions:", final_predictions)

from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(test_labels, final_predictions)
print(f"Accuracy: {accuracy * 100:.2f}%")
report = classification_report(test_labels, final_predictions, target_names=["Mild", "Severe"])
print(report)

clip_predictions = classify_with_threshold(clip_test_features, clip_mild_prototype, clip_severe_prototype, threshold=0.5)
blip_predictions = classify_with_threshold(blip_test_features, blip_mild_prototype, blip_severe_prototype, threshold=0.5)
flava_predictions = classify_with_threshold(flava_test_features, flava_mild_prototype, flava_severe_prototype, threshold=0.5)
final_predictions = weighted_voting(clip_predictions, blip_predictions, flava_predictions, weights=[2,6,1])

accuracy = accuracy_score(test_labels, final_predictions)
print(f"Weighted Voting Accuracy: {accuracy * 100:.2f}%")
report = classification_report(test_labels, final_predictions, target_names=["Mild", "Severe"])
print(report)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

conf_matrix = confusion_matrix(test_labels, final_predictions)

disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=["Mild", "Severe"])
disp.plot(cmap="Blues")
disp.ax_.set_title("Confusion Matrix")
disp.ax_.set_xlabel("Predicted Labels")
disp.ax_.set_ylabel("True Labels")
disp.figure_.tight_layout()





