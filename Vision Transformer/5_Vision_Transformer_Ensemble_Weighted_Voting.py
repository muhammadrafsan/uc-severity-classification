# -*- coding: utf-8 -*-
"""5-ensemble-vit-new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gKls38MF3MrO4ZKQsAV0zDPmQ1WOVgqv
"""

!pip install timm

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torchvision.datasets import ImageFolder
import timm
import numpy as np
import copy
from sklearn.metrics import accuracy_score, classification_report
from torch.cuda.amp import GradScaler, autocast

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

data_dir = '/kaggle/input/ulcerative-colitis-binary-dataset/Binary Ulceraive Colitis'

transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

dataset = ImageFolder(root=data_dir, transform=transform)


train_size = int(0.7 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size


train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])


batch_size = 16
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)


print(f'Training set size: {len(train_set)}')
print(f'Validation set size: {len(val_set)}')
print(f'Test set size: {len(test_set)}')

def modify_and_finetune_vit(model_name, num_classes=2, unfreeze_layers=12):
    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)

    for param in list(model.parameters())[-unfreeze_layers:]:
        param.requires_grad = True

    return model.to(device)

vit_base = modify_and_finetune_vit('vit_base_patch16_224', num_classes=2)
vit_large = modify_and_finetune_vit('vit_large_patch16_224', num_classes=2)
swin_base = modify_and_finetune_vit('swin_base_patch4_window7_224', num_classes=2)
deit_base = modify_and_finetune_vit('deit_base_patch16_224', num_classes=2)
beit_base = modify_and_finetune_vit('beit_base_patch16_224', num_classes=2)

models_dict = {
    'ViT_Base': vit_base,
    'ViT_Large': vit_large,
    'Swin_Base': swin_base,
    'DeiT_Base': deit_base,
    'BEiT_Base': beit_base
}

def train_model_with_finetuning(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3, accumulation_steps=2):
    best_acc = 0.0
    best_model_wts = copy.deepcopy(model.state_dict())
    early_stop_counter = 0
    scaler = GradScaler()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        running_corrects = 0

        optimizer.zero_grad()
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            with autocast():
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels) / accumulation_steps

            scaler.scale(loss).backward()

            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        model.eval()
        val_corrects = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                val_corrects += torch.sum(preds == labels.data)

        val_acc = val_corrects.double() / len(val_loader.dataset)

        if val_acc > best_acc:
            best_acc = val_acc
            best_model_wts = copy.deepcopy(model.state_dict())
            early_stop_counter = 0
        else:
            early_stop_counter += 1

        print(f'Epoch {epoch+1}/{num_epochs} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}')

        if early_stop_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}. Best Val Acc: {best_acc:.4f}.")
            break

    model.load_state_dict(best_model_wts)
    return model

criterion = nn.CrossEntropyLoss()
num_epochs = 200
patience = 50
weight_decay = 1e-4

for name, model in models_dict.items():
    print(f'\nTraining {name} model with fine-tuning...')
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.000001, weight_decay=weight_decay)
    models_dict[name] = train_model_with_finetuning(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)

def get_softmax_probs(model, test_loader):
    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)

            # Apply softmax to get probabilities
            probs = F.softmax(outputs, dim=1).cpu().numpy()
            all_probs.append(probs)

            # Store true labels
            all_labels.extend(labels.numpy())

    all_probs = np.vstack(all_probs)
    return all_probs, np.array(all_labels)

model_softmax_probs = {}
for name, model in models_dict.items():
    print(f'Generating softmax probabilities for {name} model...')
    model_softmax_probs[name], test_labels = get_softmax_probs(model, test_loader)


weights = [0.25, 0.2, 0.25, 0.15, 0.15]
weighted_sum_probs = np.zeros_like(list(model_softmax_probs.values())[0])

for i, (name, probs) in enumerate(model_softmax_probs.items()):
    weighted_sum_probs += weights[i] * probs

final_probs = weighted_sum_probs / sum(weights)
final_preds = np.argmax(final_probs, axis=1)

ensemble_accuracy = accuracy_score(test_labels, final_preds)
print(f'Ensemble Accuracy: {ensemble_accuracy:.4f}')


print('\nClassification Report:')
print(classification_report(test_labels, final_preds, target_names=dataset.classes))









